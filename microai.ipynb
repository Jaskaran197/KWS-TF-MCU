{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhogal\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\bhogal\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\bhogal\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\bhogal\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\bhogal\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\bhogal\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\bhogal\\.conda\\envs\\keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\bhogal\\.conda\\envs\\keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\bhogal\\.conda\\envs\\keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\bhogal\\.conda\\envs\\keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\bhogal\\.conda\\envs\\keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\bhogal\\.conda\\envs\\keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# pylint: disable=unused-import\n",
    "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "# pylint: enable=unused-import\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def load_graph(filename):\n",
    "  \"\"\"Unpersists graph from file as default graph.\"\"\"\n",
    "  with tf.gfile.FastGFile(filename, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "  \"\"\"Read in labels, one label per line.\"\"\"\n",
    "  return [line.rstrip() for line in tf.gfile.GFile(filename)]\n",
    "\n",
    "\n",
    "def run_graph(wav_data, labels, input_layer_name, output_layer_name,\n",
    "              num_top_predictions):\n",
    "  \"\"\"Runs the audio data through the graph and prints predictions.\"\"\"\n",
    "  with tf.Session() as sess:\n",
    "    # Feed the audio data as input to the graph.\n",
    "    #   predictions  will contain a two-dimensional array, where one\n",
    "    #   dimension represents the input image count, and the other has\n",
    "    #   predictions per class\n",
    "    softmax_tensor = sess.graph.get_tensor_by_name(output_layer_name)\n",
    "    #print(wav_data)\n",
    "    predictions, = sess.run(softmax_tensor, {input_layer_name: wav_data})\n",
    "    #print(len(predictions))\n",
    "    # Sort to show labels in order of confidence\n",
    "    top_k = predictions.argsort()[-num_top_predictions:][::-1]\n",
    "    for node_id in top_k:\n",
    "      human_string = labels[node_id]\n",
    "      score = predictions[node_id]\n",
    "      \n",
    "      print('%s (score = %.5f)' % (human_string, score))\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def label_wav(wav, labels, graph, input_name, output_name, how_many_labels):\n",
    "  \"\"\"Loads the model and labels, and runs the inference to print predictions.\"\"\"\n",
    "  if not wav or not tf.gfile.Exists(wav):\n",
    "    tf.logging.fatal('Audio file does not exist %s', wav)\n",
    "\n",
    "  if not labels or not tf.gfile.Exists(labels):\n",
    "    tf.logging.fatal('Labels file does not exist %s', labels)\n",
    "\n",
    "  if not graph or not tf.gfile.Exists(graph):\n",
    "    tf.logging.fatal('Graph file does not exist %s', graph)\n",
    "\n",
    "  labels_list = load_labels(labels)\n",
    "\n",
    "  # load graph, which is stored in the default session\n",
    "  load_graph(graph)\n",
    "\n",
    "  with open(wav, 'rb') as wav_file:\n",
    "    wav_data = wav_file.read()\n",
    "  #print(wav_data)\n",
    "  #print(len(wav_data))\n",
    "  #print(type(wav_data))\n",
    "\n",
    "  #print(labels_list)\n",
    "  #print(input_name, output_name, how_many_labels)\n",
    "  run_graph(wav_data, labels_list, input_name, output_name, how_many_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_wav('silence.wav','Pretrained_models\\labels.txt','DNN_S.pb','wav_data:0','labels_softmax:0',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_wav_data(wav, labels, graph, input_name, output_name, how_many_labels):\n",
    "  \"\"\"Loads the model and labels, and runs the inference to print predictions.\"\"\"\n",
    "  if not wav or not tf.gfile.Exists(wav):\n",
    "    tf.logging.fatal('Audio file does not exist %s', wav)\n",
    "\n",
    "  if not labels or not tf.gfile.Exists(labels):\n",
    "    tf.logging.fatal('Labels file does not exist %s', labels)\n",
    "\n",
    "  if not graph or not tf.gfile.Exists(graph):\n",
    "    tf.logging.fatal('Graph file does not exist %s', graph)\n",
    "\n",
    "  labels_list = load_labels(labels)\n",
    "\n",
    "  # load graph, which is stored in the default session\n",
    "  load_graph(graph)\n",
    "\n",
    "\n",
    "\n",
    "  print(labels_list)\n",
    "  print(input_name, output_name, how_many_labels)\n",
    "  run_graph(wav, labels_list, input_name, output_name, how_many_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "Triggered\n",
      "WARNING:tensorflow:From <ipython-input-2-e8d771eaf096>:19: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n",
      "left (score = 0.34399)\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from array import array\n",
    "\n",
    "FORMAT=pyaudio.paInt16\n",
    "CHANNELS=1\n",
    "RATE=16000\n",
    "CHUNK=1024\n",
    "RECORD_SECONDS=1\n",
    "FILE_NAME=\"RECORDING.wav\"\n",
    "\n",
    "audio=pyaudio.PyAudio() #instantiate the pyaudio\n",
    "\n",
    "#recording prerequisites\n",
    "stream=audio.open(format=FORMAT,channels=CHANNELS, \n",
    "                  rate=RATE,\n",
    "                  input=True,\n",
    "                  frames_per_buffer=CHUNK)\n",
    "\n",
    "#starting recording\n",
    "frames=[]\n",
    "while(True):\n",
    "    data=stream.read(CHUNK)\n",
    "    data_chunk=array('h',data)\n",
    "    vol=max(data_chunk)\n",
    "    print('.')\n",
    "    if(vol>=1000):\n",
    "        print('Triggered')\n",
    "        break\n",
    "for i in range(0,int(RATE/CHUNK*RECORD_SECONDS)):\n",
    "    data=stream.read(CHUNK)\n",
    "    data_chunk=array('h',data)\n",
    "    vol=max(data_chunk)\n",
    "    #if(vol>=300):\n",
    "     #   print(\"something said\")\n",
    "    frames.append(data)\n",
    "    #else:\n",
    "        #print(\"nothing\")\n",
    "    #print(\"\\n\")\n",
    "\n",
    "\n",
    "#end of recording\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "#writing to file\n",
    "wavfile=wave.open(FILE_NAME,'wb')\n",
    "wavfile.setnchannels(CHANNELS)\n",
    "wavfile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "wavfile.setframerate(RATE)\n",
    "wavfile.writeframes(b''.join(frames))#append frames recorded to file\n",
    "wavfile.close()\n",
    "\n",
    "label_wav('RECORDING.wav','Pretrained_models\\labels.txt','DNN_S.pb','wav_data:0','labels_softmax:0',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triggered\n",
      "on (score = 0.22909)\n",
      "Triggered\n",
      "left (score = 0.63180)\n",
      "Triggered\n",
      "on (score = 0.76567)\n",
      "Triggered\n",
      "on (score = 0.64561)\n",
      "Triggered\n",
      "right (score = 0.92620)\n",
      "Triggered\n",
      "down (score = 0.41801)\n",
      "Triggered\n",
      "left (score = 0.85192)\n",
      "Triggered\n",
      "yes (score = 0.39493)\n",
      "Triggered\n",
      "left (score = 0.24423)\n",
      "Triggered\n",
      "right (score = 0.53452)\n",
      "Triggered\n",
      "right (score = 0.60255)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a195e89a3e3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mframes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mold_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCHUNK_THRESH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mdata_chunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mold_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mvol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_chunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\pyaudio.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    606\u001b[0m                           paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_on_overflow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_read_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from array import array\n",
    "\n",
    "\n",
    "\n",
    "FORMAT=pyaudio.paInt16\n",
    "CHANNELS=1\n",
    "RATE=16000\n",
    "CHUNK=1024\n",
    "CHUNK_THRESH=64\n",
    "RECORD_SECONDS=1\n",
    "FILE_NAME=\"RECORDING.wav\"\n",
    "while(True):\n",
    "    audio=pyaudio.PyAudio() #instantiate the pyaudio\n",
    "\n",
    "    #recording prerequisites\n",
    "    stream=audio.open(format=FORMAT,channels=CHANNELS, \n",
    "                      rate=RATE,\n",
    "                      input=True,\n",
    "                      frames_per_buffer=CHUNK)\n",
    "\n",
    "    #starting recording\n",
    "    frames=[]\n",
    "    while(True):\n",
    "        old_data=stream.read(CHUNK_THRESH)\n",
    "        data_chunk=array('h',old_data)\n",
    "        vol=max(data_chunk)\n",
    "        #print(old_data)\n",
    "        if(vol>=600):\n",
    "            print('Triggered')\n",
    "            #frames.append(old_data)\n",
    "            break\n",
    "    for i in range(0,int(RATE/CHUNK*RECORD_SECONDS)):\n",
    "        #frames.append(old_data)\n",
    "        data=stream.read(CHUNK)\n",
    "        data_chunk=array('h',data)\n",
    "        vol=max(data_chunk)\n",
    "        #if(vol>=300):\n",
    "         #   print(\"something said\")\n",
    "        frames.append(data)\n",
    "        #else:\n",
    "            #print(\"nothing\")\n",
    "        #print(\"\\n\")\n",
    "\n",
    "\n",
    "    #end of recording\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    #writing to file\n",
    "    wavfile=wave.open(FILE_NAME,'wb')\n",
    "    wavfile.setnchannels(CHANNELS)\n",
    "    wavfile.setsampwidth(2)\n",
    "    wavfile.setframerate(RATE)\n",
    "    wavfile.writeframes(b''.join(frames))#append frames recorded to file\n",
    "    wavfile.close()\n",
    "\n",
    "    label_wav('RECORDING.wav','Pretrained_models\\labels.txt','DNN_S.pb','wav_data:0','labels_softmax:0',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right (score = 0.60255)\n"
     ]
    }
   ],
   "source": [
    "label_wav('RECORDING.wav','Pretrained_models\\labels.txt','DNN_S.pb','wav_data:0','labels_softmax:0',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "with wave.open('silence.wav', \"rb\") as wave_file:\n",
    "        frame_rate = wave_file.getframerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_file = wave.open('silence.wav', \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chunk.Chunk at 0x2c753a30470>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave_file.getfp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_wav_data(frames[0],'Pretrained_models\\labels.txt','DNN_S.pb','wav_data:0','labels_softmax:0',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.audio.decode_wav(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'RIFF$}\\x00\\x00WAVEfmt '+str(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from array import array\n",
    "\n",
    "FORMAT=pyaudio.paInt16\n",
    "CHANNELS=1\n",
    "RATE=16000\n",
    "CHUNK=1024\n",
    "RECORD_SECONDS=2\n",
    "FILE_NAME=\"RECORDING.wav\"\n",
    "\n",
    "audio=pyaudio.PyAudio() #instantiate the pyaudio\n",
    "\n",
    "#recording prerequisites\n",
    "stream=audio.open(format=FORMAT,channels=CHANNELS, \n",
    "                  rate=RATE,\n",
    "                  input=True,\n",
    "                  frames_per_buffer=CHUNK)\n",
    "\n",
    "#starting recording\n",
    "frames=[]\n",
    "\n",
    "for i in range(0,int(RATE/CHUNK*RECORD_SECONDS)):\n",
    "    data=stream.read(CHUNK)\n",
    "    \n",
    "    data_chunk=array('h',data)\n",
    "    vol=max(data_chunk)\n",
    "    #if(vol>=300):\n",
    "     #   print(\"something said\")\n",
    "    frames.append(data)\n",
    "    #else:\n",
    "        #print(\"nothing\")\n",
    "    #print(\"\\n\")\n",
    "\n",
    "\n",
    "#end of recording\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "#writing to file\n",
    "print(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wait in silence to begin recording; wait in silence to terminate\")\n",
    "record_to_file('demo.wav')\n",
    "print(\"done - result written to demo.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import sys\n",
    "\n",
    "chunk = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "RECORD_SECONDS = 1\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS, \n",
    "                rate=RATE, \n",
    "                input=True,\n",
    "                output=True,\n",
    "                frames_per_buffer=chunk)\n",
    "\n",
    "print (\"* recording\")\n",
    "for i in range(0, int(44100 / chunk * RECORD_SECONDS)):\n",
    "    data = stream.read(chunk)\n",
    "    # check for silence here by comparing the level with 0 (or some threshold) for \n",
    "    # the contents of data.\n",
    "    # then write data or not to a file\n",
    "\n",
    "print (\"* done\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "with open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_wav(data,'Pretrained_models\\labels.txt','DNN_L.pb','wav_data:0','labels_softmax:0',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list=['_silence_', '_unknown_', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "input_name='wav_data:0' \n",
    "output_name='labels_softmax:0'\n",
    "how_many_labels=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_to_file('demo.wav')\n",
    "print(\"done - result written to demo.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('silence.wav', 'rb') as wav_file:\n",
    "    wav_data = wav_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "SILENCE_LIMIT = 1  # Silence limit in seconds. The max ammount of seconds where\n",
    "                   # only silence is recorded. When this time passes the\n",
    "                   # recording finishes and the file is delivered.\n",
    "\n",
    "PREV_AUDIO = 0.2  # Previous audio (in seconds) to prepend. When noise\n",
    "                  # is detected, how much of previously recorded audio is\n",
    "                  # prepended. This helps to prevent chopping the beggining\n",
    "                  # of the phrase.\n",
    "import math\n",
    "import audioop\n",
    "import time\n",
    "from collections import deque\n",
    "def listen_for_speech(threshold=400, num_phrases=30):\n",
    "    THRESHOLD=400\n",
    "    \"\"\"\n",
    "    Listens to Microphone, extracts phrases from it and sends it to \n",
    "    Google's TTS service and returns response. a \"phrase\" is sound \n",
    "    surrounded by silence (according to threshold). num_phrases controls\n",
    "    how many phrases to process before finishing the listening process \n",
    "    (-1 for infinite). \n",
    "    \"\"\"\n",
    "\n",
    "    #Open stream\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print (\"* Listening mic. \")\n",
    "    audio2send = []\n",
    "    cur_data = ''  # current chunk  of audio data\n",
    "    rel = RATE/CHUNK\n",
    "    slid_win = deque(maxlen=int(SILENCE_LIMIT * rel))\n",
    "    #Prepend audio from 0.5 seconds before noise was detected\n",
    "    prev_audio = deque(maxlen=int(PREV_AUDIO * rel)) \n",
    "    started = False\n",
    "    n = num_phrases\n",
    "    response = []\n",
    "\n",
    "    while (num_phrases == -1 or n > 0):\n",
    "        cur_data = stream.read(CHUNK)\n",
    "        slid_win.append(math.sqrt(abs(audioop.avg(cur_data, 4))))\n",
    "        #print slid_win[-1]\n",
    "        if(sum([x > THRESHOLD for x in slid_win]) > 0):\n",
    "            if(not started):\n",
    "                print (\"Starting record of phrase\")\n",
    "                started = True\n",
    "            audio2send.append(cur_data)\n",
    "        elif (started is True):\n",
    "            print (\"Finished\")\n",
    "            # The limit was reached, finish capture and deliver.\n",
    "            filename = save_speech(list(prev_audio) + audio2send, p)\n",
    "            label_wav(filename,'Pretrained_models\\labels.txt','DNN_S.pb','wav_data:0','labels_softmax:0',1)\n",
    "\n",
    "            # Send file to Google and get response\n",
    "            #r = stt_google_wav(filename) \n",
    "            #if num_phrases == -1:\n",
    "                #print (\"Response\", r)\n",
    "            #else:\n",
    "                #response.append(r)\n",
    "            # Remove temp file. Comment line to review.\n",
    "            os.remove(filename)\n",
    "            # Reset all\n",
    "            started = False\n",
    "            slid_win = deque(maxlen=int(SILENCE_LIMIT * rel))\n",
    "            prev_audio = deque(maxlen=int(0.5 * rel)) \n",
    "            audio2send = []\n",
    "            n -= 1\n",
    "            print (\"Listening ...\")\n",
    "        else:\n",
    "            prev_audio.append(cur_data)\n",
    "\n",
    "    print (\"* Done recording\")\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def save_speech(data, p):\n",
    "    \"\"\" Saves mic data to temporary WAV file. Returns filename of saved \n",
    "        file \"\"\"\n",
    "\n",
    "    filename = 'output_'+str(int(time.time()))\n",
    "    # writes data to WAV file\n",
    "    #data = ''.join(data)\n",
    "    \n",
    "    wf = wave.open(filename + '.wav', 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "    wf.setframerate(16000)  # TODO make this value a function parameter?\n",
    "    #wf.writeframes(data)\n",
    "    wf.writeframes(b''.join(data))\n",
    "    wf.close()\n",
    "    return filename + '.wav'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Listening mic. \n",
      "Starting record of phrase\n",
      "Finished\n",
      "on (score = 0.70106)\n",
      "Listening ...\n",
      "Starting record of phrase\n",
      "Finished\n",
      "off (score = 0.99679)\n",
      "Listening ...\n",
      "Starting record of phrase\n",
      "Finished\n",
      "right (score = 0.98753)\n",
      "Listening ...\n",
      "Starting record of phrase\n",
      "Finished\n",
      "left (score = 0.52431)\n",
      "Listening ...\n",
      "Starting record of phrase\n",
      "Finished\n",
      "right (score = 0.99920)\n",
      "Listening ...\n",
      "Starting record of phrase\n",
      "Finished\n",
      "left (score = 0.97932)\n",
      "Listening ...\n",
      "Starting record of phrase\n",
      "Finished\n",
      "off (score = 0.99714)\n",
      "Listening ...\n",
      "Starting record of phrase\n",
      "Finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-2a45907b1d8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlisten_for_speech\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-82-c557f3f130c2>\u001b[0m in \u001b[0;36mlisten_for_speech\u001b[1;34m(threshold, num_phrases)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;31m# The limit was reached, finish capture and deliver.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_speech\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_audio\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0maudio2send\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mlabel_wav\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Pretrained_models\\labels.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DNN_S.pb'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wav_data:0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'labels_softmax:0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;31m# Send file to Google and get response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e8d771eaf096>\u001b[0m in \u001b[0;36mlabel_wav\u001b[1;34m(wav, labels, graph, input_name, output_name, how_many_labels)\u001b[0m\n\u001b[0;32m     75\u001b[0m   \u001b[1;31m#print(labels_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m   \u001b[1;31m#print(input_name, output_name, how_many_labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m   \u001b[0mrun_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwav_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow_many_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e8d771eaf096>\u001b[0m in \u001b[0;36mrun_graph\u001b[1;34m(wav_data, labels, input_layer_name, output_layer_name, num_top_predictions)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0msoftmax_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_layer_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m#print(wav_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minput_layer_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwav_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;31m#print(len(predictions))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# Sort to show labels in order of confidence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1339\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m   1341\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m   \u001b[1;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "listen_for_speech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "  \"\"\"Entry point for script, converts flags to arguments.\"\"\"\n",
    "  label_wav(FLAGS.wav, FLAGS.labels, FLAGS.graph, FLAGS.input_name,\n",
    "            FLAGS.output_name, FLAGS.how_many_labels)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--wav', type=str, default='', help='Audio file to be identified.')\n",
    "  parser.add_argument(\n",
    "      '--graph', type=str, default='', help='Model to use for identification.')\n",
    "  parser.add_argument(\n",
    "      '--labels', type=str, default='', help='Path to file containing labels.')\n",
    "  parser.add_argument(\n",
    "      '--input_name',\n",
    "      type=str,\n",
    "      default='wav_data:0',\n",
    "      help='Name of WAVE data input node in model.')\n",
    "  parser.add_argument(\n",
    "      '--output_name',\n",
    "      type=str,\n",
    "      default='labels_softmax:0',\n",
    "      help='Name of node outputting a prediction in the model.')\n",
    "  parser.add_argument(\n",
    "      '--how_many_labels',\n",
    "      type=int,\n",
    "      default=3,\n",
    "      help='Number of results to show.')\n",
    "\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (keras)",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
